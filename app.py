import streamlit as st
import fitz  # PyMuPDF
import json
import requests

st.set_page_config(page_title="Agent IA - Appel √† projet", layout="centered")

st.title("üìÑ Agent IA - Analyse d'un appel √† projet (version gratuite)")

# Chargement du profil associatif
try:
    with open("profil_association.json", "r", encoding="utf-8") as f:
        profil = json.load(f)
except FileNotFoundError:
    st.error("Fichier 'profil_association.json' manquant.")
    st.stop()

# Entr√©e du token Hugging Face
hf_token = st.text_input("üîë Token Hugging Face (ne sera pas stock√©)", type="password")

# T√©l√©versement du PDF
uploaded_file = st.file_uploader("üìé T√©l√©verser un appel √† projet (PDF)", type="pdf")

def lire_pdf(file):
    doc = fitz.open(stream=file.read(), filetype="pdf")
    texte = ""
    for page in doc:
        texte += page.get_text()
    return texte

import time

def interroger_modele_hf(prompt, token):
    API_URL = "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta"
    headers = {"Authorization": f"Bearer {token}"}
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": 512,
            "temperature": 0.7
        }
    }

    try:
        response = requests.post(API_URL, headers=headers, json=payload)

        if response.status_code == 503:
            # Mod√®le en train de se charger
            st.warning("‚è≥ Le mod√®le se r√©veille, r√©essai dans 10 secondes...")
            time.sleep(10)
            response = requests.post(API_URL, headers=headers, json=payload)

        if response.status_code == 200:
            resultat = response.json()
            if isinstance(resultat, list) and "generated_text" in resultat[0]:
                return resultat[0]["generated_text"]
            else:
                return "‚ö†Ô∏è R√©ponse inattendue du mod√®le."
        else:
            return f"‚ùå Erreur Hugging Face : code {response.status_code}"

    except Exception as e:
        return f"‚ùå Erreur syst√®me : {str(e)}"


if uploaded_file and hf_token:
    texte_pdf = lire_pdf(uploaded_file)

    with st.spinner("üîç Analyse en cours..."):
       prompt = f"""
    Tu es un assistant expert en r√©daction d'appels √† projets associatifs.

    Voici un appel √† projet :
    -------------------------
    {texte_pdf}

    Voici le profil de l'association :
    -------------------------
    {json.dumps(profil, indent=2)}

    Ta t√¢che : 
    Propose une **r√©ponse structur√©e** √† cet appel √† projet au nom de l'association. Ne fais **aucune analyse**, ne donne pas d'avis, ne fais pas de r√©sum√©.

    Contenu attendu :
    - Titre du projet
    - Objectifs du projet
    - Public vis√©
    - Activit√©s pr√©vues
    - Partenaires √©ventuels
    - Budget estim√© (si possible)
    - R√©sultats attendus

    R√©dige de fa√ßon professionnelle, claire et concise.
"""



resultat = interroger_modele_hf(prompt, hf_token)
st.subheader("üìå R√©sultat de l'analyse")
st.markdown(resultat)
