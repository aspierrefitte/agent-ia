import streamlit as st
import requests
import PyPDF2
import json
import time

st.set_page_config(page_title="Agent IA pour appel √† projet", page_icon="üìÑ")

st.title("üìÑ Agent IA - R√©daction d'appel √† projet")
st.write("D√©posez un appel √† projet (PDF) et le profil de votre association (JSON). L'IA g√©n√®re une proposition adapt√©e.")

# Interface utilisateur
hf_token = st.text_input("üîë Token Hugging Face", type="password")
uploaded_file = st.file_uploader("üìé Appel √† projet (PDF)", type="pdf")


# Lire un fichier PDF
def lire_pdf(fichier):
    lecteur = PyPDF2.PdfReader(fichier)
    texte = ""
    for page in lecteur.pages:
        texte += page.extract_text()
    return texte
    
# Chargement du profil associatif
try:
    with open("profil_association.json", "r", encoding="utf-8") as f:
        profil = json.load(f)
except FileNotFoundError:
    st.error("Fichier 'profil_association.json' manquant.")
    st.stop()

# Appeler le mod√®le Hugging Face
def interroger_modele_hf(prompt, token):
    API_URL = "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta"
    headers = {"Authorization": f"Bearer {token}"}
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": 512,
            "temperature": 0.7
        }
    }

    try:
        response = requests.post(API_URL, headers=headers, json=payload)

        if response.status_code == 503:
            st.warning("‚è≥ Le mod√®le se r√©veille, r√©essai dans 10 secondes...")
            time.sleep(10)
            response = requests.post(API_URL, headers=headers, json=payload)

        if response.status_code == 200:
            resultat = response.json()
            if isinstance(resultat, list):
                return resultat[0].get("generated_text", "‚ö†Ô∏è R√©ponse vide.")
            else:
                return f"R√©ponse inattendue : {resultat}"
        else:
            return f"‚ùå Erreur Hugging Face : code {response.status_code}"

    except Exception as e:
        return f"‚ùå Erreur syst√®me : {str(e)}"

# Nettoyer la r√©ponse IA pour n'afficher que le projet
def extraire_reponse(text):
    index = text.find("Titre du projet")
    if index != -1:
        return text[index:]
    return text

# Traitement principal
if uploaded_file and hf_token :
    texte_pdf = lire_pdf(uploaded_file)

    prompt = f"""
Tu es un assistant expert en r√©daction d'appels √† projets associatifs.

Voici un appel √† projet :
-------------------------
{texte_pdf}

Voici le profil de l'association :
-------------------------
{json.dumps(profil, indent=2)}

Ta t√¢che : 
Propose une **r√©ponse structur√©e** √† cet appel √† projet au nom de l'association. Ne fais **aucune analyse**, ne donne pas d'avis, ne fais pas de r√©sum√©.

Contenu attendu :
- Titre du projet
- Objectifs du projet
- Public vis√©
- Activit√©s pr√©vues
- Partenaires √©ventuels
- Budget estim√© (si possible)
- R√©sultats attendus

R√©dige de fa√ßon professionnelle, claire et concise.
"""

    with st.spinner("‚úçÔ∏è G√©n√©ration de la r√©ponse..."):
        resultat = interroger_modele_hf(prompt, hf_token)
        st.subheader("üìÑ Proposition de projet g√©n√©r√©e")
        st.markdown(resultat)
